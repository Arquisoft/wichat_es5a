ifndef::imagesdir[:imagesdir: ../images]

[[section-testing]]
== Test

=== Test unitarios
Hemos realizado diversos test para cada servicio de la aplicación usando Jest y Testing Library de React. Para cada componente del Webapp por ejemplo hemos creado una clase test propia para probar sus funcionalidades, así como para cada servicio del backend.

Este ha sido un proceso que hemos desarrollado a lo largo del proyecto y del cual nos hemos ayudado de la herramiennta SonarCloud que analiza el código de nuestro repositorio de GitHub para decirnos cual es nuestro porcentaje de código cubierto por test, así como las posibles brechas de seguridad que puede tener la aplicación(muchas de estas falsos positivos).

Aunque el porcentaje pueda variar un poco al final del proyecto hemos conseguido superar un 80% de coverage para el código global de la aplicación. Requisito necesario para que la aplicación tenga suficientes test.

image::Test_unitarios.png["Test unitarios"]

=== Test de Usabilidad

Hemos realizado dos tandas con usuarios tanto en el sprint 3 como en el sprint 4 para que probaran la aplicación desplegada.

Estos usuarios eran personas jóvenes de entre 20 y 30 años de edad, bastante familiarizados con las aplicaciones web en general, por lo que la curva de aprendizaje fue bastante rápida y no supuso un gran problema.

Algunos de los problemas que encontraron los usuarios y que nos parecieron relevantes fueron:

==== Sprint 3
* Las preguntas en difícil tardan bastante en generar(1 minuto).
* La ventana del chat tapa su propio botón de cerrar.
* El botón siguiente pregunta se va desplazando hacia abajo con cada click.
* La primera vez que le das al botón de iniciar sesión no va, tienes que darle dos veces.

==== Sprint 4
* El botón del LLM no era intuitivo para su uso, haciendo que el usuario no supiera si estaba desactivado.
* Algunas preguntas de comida enseñaban códigos en vez de respuestas.
* Un bug en el modo supervivencia que hacía que se repitiese la misma pregunta a partir de la 20.

=== Test E2E
Para la realización de test End to End hemos usado el framework de Cucumber. Los test E2E son pruebas automatizadas que validan el funcionamiento de una aplicación desde el punto de vista del usuario final. Su objetivo es comprobar que todos los componentes del sistema como el front, back, base de datos y demás, funcionen juntos como se espera.

Nosotros hemos implementado 3 features a los test, siendo estas:
* **Game:**Simulará al jugador jugando juegos en distintas dificultades e interactuando con el LLM mientras juega.
* **Login-form:**Simulará el formulario de registro y los posibles errores que puede cometer el usuario al meter datos incorrectos o no estar registrado.
* **Register-form:**Simulará el formulario de identificación de usuario y los posibles errores que puede cometer el usuario al meter datos incorrectos.

=== Test de carga
Para realizar los test de carga hemos usado la herramienta Gatling, usando el lenguaje scala para crear los distintos programas y haciendo uso de la funcionalidad de recorder de Gatling para "grabar" los movimientos de un usuario por la aplicación y luego adaptarlos a los test de carga.

Hemos probado los test de carga con la aplicación deplegada, ya que si lo hicieramos en local, se tendrían en cuenta los componentes de nuestro sistema. Además se ha creado una función en los programas que aleatoriza la creación de usuarios, siendo casi imposible que dos usuarios tengan el mismo nombre, correo u contraseña(aunque esta última podría ser igual).

Los pasos que hemos grabado han sido los siguientes: Identificar un nuevo usuario en la aplicación, después hacer login, jugar a un juego en dificultad fácil de 5 preguntas, pedirle una pista al LLM entre medias y por último, guardar la puntuación del juego y mirar el historial. Hemos realizado dos pruebas, una para probar que la aplicación puede aguantar un nivel moderado de usuarios concurrentes y otra exhaustiva para poder ver los cuellos de botella de la aplicación y el margen de mejora.

==== 120 usuarios en 60 segundos
Hay que aclarar antes de este punto, que teníamos un diagrama de 120 usuarios con todas las peticiones OK y en verde, pero por malas praxis, acabamos perdiendolo, lo que pasa es que al volver a ejecutar el programa había un apagón en la zona, que hizo que incluso el día despúes el wifi no fuera. Por lo que al ejecutarse con mala conexión hace timeout en 11 peticiones y eleva el tiempo de las demás.

Podemos ver que en este test que casi todas las peticiones se realizan en un tiempo razonable, habiendo bastante pocas peticiones que tarden bastante, con esto podemos comprobar que la aplicación es estable con un número adecuado de usuarios que no debería darnos problema a la hora de entregar el proyecto al solicitante.

image::Carga1.png["Carga 1"]

image::Carga2.png["Carga 2"]

link:../../gatling/Resultados/Prueba_120_usuarios/index.html[Ver resultados de rendimiento primer esquema]

==== 1800 usuarios en 60 segundos
Aquí podemos ya ver bastantes errores graves, un cuarto de las peticiones fallan ante tal cantidad de usuarios.

El problema principal está en que el backend, se satura de usuarios y deja de funcionar. Haciendo que peticiones como AddUser o Login dejen de funcionar, pero otros servicios como la petición al LLM o la generación de preguntas con WikiData aguantan bastante bien. También cabe destacar el 100% de error a la hora de la entrega de la css de la aplicación, lo que genera que ningún usuario pueda ver la aplicación con normalidad. 

El problema creemos que está en la potencia de nuestra máquina virtual, la cual tendríamos que cambiar por una más potente, con mejor memoria y rendimiento para que el backend pudiera aguantar más carga de usuarios.

image::Carga3.png["Carga 3"]

image::Carga4.png["Carga 4"]

link:../../gatling/Resultados/Prueba_1800_usuarios/index.html[Ver resultados de rendimiento segundo esquema]

=== Monitorización
También hemos realizado monitorización de la aplicación usando las herramientas de Prometheus y Grafana para observar el comportamiento en tiempo real de la aplicación y asegurarnos de que funcione correctamente, detectando problemas rápidamente y mejorando su rendimiento.

Monitorizar una aplicación nos sirve para:

* Garantizar buena disponibilidad.
* Mejorar el rendimiento.
* Detectar errores de forma temprana.
* Controlar el recorrido de los usuarios.
* Generar alertas en caso de fallo.

Al final, el objetivo es anticiparse a los problemas y mejorar continuamente la calidad y eficiencia de la aplicación.

En las gráficas obtenidas, hemos analizado tres métricas clave:

* **Tasa de Solicitudes por Minuto (Rate)**: Esta métrica muestra la cantidad de solicitudes procesadas por minuto. Una tasa constante indica estabilidad en el sistema, mientras que picos o caídas pueden reflejar eventos específicos, como un aumento en los usuarios concurrentes o problemas en el backend.

* **Peticiones fallidas por Minuto (Errors)**: Representa la cantidad de solicitudes fallidas. Un aumento en los errores puede indicar saturación del backend, problemas de conectividad con otros servicios o errores en la lógica de negocio.

* **Duración de las Solicitudes (Duration)**: Muestra el tiempo promedio que tarda el sistema en procesar una solicitud. Un aumento en esta métrica puede ser un signo de saturación o problemas de rendimiento en el backend. Picos en la duración podrían coincidir con un aumento en la carga o errores en otros servicios.

image::Grafana.png["Monitorización"]

En esta imagen podemos observar:

*  **Una fluctuación en el número de peticiones **. Esto tiene sentido ya que el número de peticiones no es un valor fijo, sino que depende del afluente de usuarios y sus acciones en la aplicación. En esta imagen, el número de peticiones ronda entre las 44 y 54 por minuto.

*  **Un importante aumento en el número de peticiones fallidas **. Esto es síntoma de errores en algún punto de la aplicación. Aunque en la imagen se observa que inicialmente hay un número importante de errores (±8), este número se reduce posteriormente hasta llegar a 2 errores, pero aumenta drásticamente hacia el final del gráfico.


*  **Estabilidad en el tiempo de respuestas **. Podemos ver que las tres gráficas mantienen una gran estabilidad con poca variación en el tiempo de respuesta. Como se observa en la gráfica, la mediana (B-series, línea verde) se mantiene prácticamente constante a lo largo del tiempo, lo que indica que la mayoría de las solicitudes son procesadas de forma consistente.

La monitorización es una gran herramienta para observar el rendimiento de la aplicación en tiempo real y poder identificar errores y corregirlos. En nuestro caso, debido a una situación ajena a nuestro control (falta de tiempo por un fallo en la red eléctrica de la región), no hemos podido realizar mejoras para paliar los problemas encontrados.
